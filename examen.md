# Oracle Cloud Infrastructure (OCI) Foundations - Examen de Práctica

## Introducción

Este documento contiene 40 preguntas de práctica para el examen **Oracle Cloud Infrastructure Foundations** con sus respectivas respuestas y explicaciones detalladas en español.

> Versión: 1.0  
> Autor: Compendio de estudio (generado)  
> Objetivo: Facilitar repaso rápido y profundo de conceptos clave de OCI Foundations con foco en AI.

## Tabla de Contenido

1. [Introducción](#introduccion)
2. [Preguntas Oficiales (Q1–Q40)](#preguntas-oficiales-q1q40)
3. [Preguntas No Oficiales](#preguntas-no-oficiales)
4. [Resumen Teórico (OCI AI Foundations)](#resumen-teorico-oci-ai-foundations)
5. [Ficha de Repaso Rápido](#ficha-de-repaso-rapido)
6. [Traducción Completa Español](#traduccion-completa-espanol)
7. [Uso y Recomendaciones de Estudio](#uso-y-recomendaciones-de-estudio)

---

### Acerca del Examen OCI Foundations

El examen OCI Foundations está diseñado para validar el conocimiento fundamental sobre Oracle Cloud Infrastructure. Cubre los siguientes temas principales:

- **Conceptos básicos de la nube**: Modelos de servicio (IaaS, PaaS, SaaS), tipos de implementación
- **Arquitectura de OCI**: Regiones, dominios de disponibilidad, dominios de fallas
- **Servicios principales**: Compute, Storage, Networking, Database
- **Identidad y gestión de acceso**: IAM, políticas, usuarios, grupos
- **Seguridad**: Cifrado, redes virtuales, firewall
- **Precios y facturación**: Modelos de precios, calculadora de costos
- **Soporte**: Niveles de soporte, SLA

---

## Preguntas del Examen
 
## Preguntas Oficiales (Q1–Q40)

En esta sección se renumeran como Q1–Q40 las 40 preguntas oficiales para estudio (derivadas de las últimas 40 identificadas). Se conserva texto original, opciones, respuesta marcada y explicación.

### Q1 (oficial)
Q: Deep Learning - What is a key feature of RNNs?
- A. Parallel processing
- B. Used for images
- C. Feedback loop retains info across time steps  ✅
- D. No internal state
Correct Answer: C
Explicación (español): Las conexiones recurrentes mantienen estado y capturan dependencias temporales en secuencias.

### Q2 (oficial)
Q: OCI Vector Search - What would you use Oracle AI Vector Search for?
- A. Store business data
- B. Manage security protocols
- C. Keyword queries
- D. Semantic queries  ✅
Correct Answer: D
Explicación (español): Búsqueda semántica mediante comparación de embeddings.

### Q3 (oficial)
Q: ML Concepts - What is 'overfitting'?
- A. Model is too simple
- B. Model learns training data too well  ✅
- C. Model ignores input data
- D. Model has no output
Correct Answer: B
Explicación (español): Memoriza datos entrenamiento y pierde generalización.

### Q4 (oficial)
Q: Which algorithm is a non-parametric approach for supervised learning?
- Linear Regression
- K-Nearest Neighbors (KNN)  ✅
- Random Forest
- Decision Trees
Correct Answer: K-Nearest Neighbors (KNN)
Explicación (español): Usa distancias sobre ejemplos almacenados sin ajustar parámetros internos.

### Q5 (oficial)
Q: What is the purpose of the Model Catalog in OCI Data Science?
- It only stores raw datasets...
- It is used to deploy models as API endpoints.
- It functions as a real-time data processing engine.
- It serves as a repository for storing, tracking, and managing machine learning models.  ✅
Correct Answer: Repositorio de gestión de modelos.
Explicación (español): Versionado, metadatos y gobernanza centralizada.

### Q6 (oficial)
Q: What technique is used to predict the price of a house based on its features?
- Regression  ✅
- Time Series Analysis
- Classification
- Clustering
Correct Answer: Regression
Explicación (español): Variable continua -> problema de regresión.

### Q7 (oficial)
Q: John... deploy for real-time predictions. Which OCI service?
- OCI Object Storage
- OCI Language
- OCI Data Science  ✅
- OCI Speech
Correct Answer: OCI Data Science
Explicación (español): Despliegue de endpoints de inferencia.

### Q8 (oficial)
Q: Which of these is NOT a common application of unsupervised ML?
- Spam detection  ✅
- Outlier detection
- Targeted marketing campaigns
- Customer segmentation
Correct Answer: Spam detection
Explicación (español): Spam se aborda como clasificación supervisada.

### Q9 (oficial)
Q: Which OCI Vision feature identifies invoice/receipt/resume?
- Table extraction
- Image classification
- Document classification  ✅
- OCR
Correct Answer: Document classification
Explicación (español): Clasifica por estructura y contenido.

### Q10 (oficial)
Q: T-Few fine-tuning reduces cost because?
- Manual config each layer
- Train entire model from scratch
- Selectively updates fraction of weights  ✅
- No customization
Correct Answer: Actualiza fracción de pesos.
Explicación (español): Parameter-efficient tuning.

### Q11 (oficial)
Q: Data type most used with deep learning?
- Human interpretable features
- Complex non-human interpretable features  ✅
- Time series data
- Only string data
Correct Answer: Datos complejos no directamente interpretables.
Explicación (español): Imágenes, audio, video, etc.

### Q12 (oficial)
Q: Normalization improves readability in OCI Speech by?
- Translate languages
- Lowercase all
- Convert numbers/dates/URLs standard formats  ✅
- Remove unnecessary words
Correct Answer: Formatos estándar.
Explicación (español): Uniformiza entidades.

### Q13 (oficial)
Q: Profanity filtering option to retain words but mark them?
- Tagging  ✅
- Removing
- Normalization
- Masking
Correct Answer: Tagging
Explicación (español): Etiqueta sin eliminar.

### Q14 (oficial)
Q: Vision feature to extract merchant/date/amount?
- OCR
- Document classification
- Key-value extraction  ✅
- Table extraction
Correct Answer: Key-value extraction
Explicación (español): Identifica pares clave:valor.

### Q15 (oficial)
Q: Difference between LLMs and traditional ML models?
- Statement about pretraining large corpus  ✅
Correct Answer: Preentrenados en gran corpus.
Explicación (español): Generalización multi-tarea.

### Q16 (oficial)
Q: Self-driving car concept applied?
- Artificial Intelligence  ✅
Correct Answer: AI
Explicación (español): Integra múltiples capacidades cognitivas.

### Q17 (oficial)
Q: Feature for per-word certainty in Speech?
- Confidence scoring  ✅
Correct Answer: Confidence scoring
Explicación (español): Puntuaciones de fiabilidad.

### Q18 (oficial)
Q: Neural network best for generating music sequences?
- RNN  ✅
Correct Answer: RNN
Explicación (español): Capta dependencias temporales.

### Q19 (oficial)
Q: Speech feature to add closed captions?
- SRT file support  ✅
Correct Answer: SRT file support
Explicación (español): Genera subtítulos estándar.

### Q20 (oficial)
Q: Guiding principles for trustworthy AI?
- Lawful, ethical, robust  ✅
Correct Answer: Legal, ética, robusta.
Explicación (español): Marco de confianza.

### Q21 (oficial)
Q: Primary function of inference process in ML?
- Predicting outcomes from new data points  ✅
Correct Answer: Predicción en nuevos datos.
Explicación (español): Uso del modelo entrenado.

### Q22 (oficial)
Q: How does Select AI generate SQL?
- Connects to LLM infers intent formulates SQL  ✅
Correct Answer: Interpreta intención y construye query.
Explicación (español): Uso de LLM para mapeo NL→SQL.

### Q23 (oficial)
Q: How does Select AI enhance interaction with Autonomous DB?
- Enables natural language prompts instead of SQL code  ✅
Correct Answer: Prompts NL.
Explicación (español): Reduce necesidad de escribir SQL.

### Q24 (oficial)
Q: Which component is NOT part of OCI AI Infrastructure?
- OCI Vault  ✅
Correct Answer: OCI Vault
Explicación (español): Servicio de llaves, no cómputo AI.

### Q25 (oficial)
Q: GPU for massive-scale HPC AI workloads?
- GB200  ✅
Correct Answer: GB200
Explicación (español): Arquitectura Grace Blackwell exascala.

### Q26 (oficial)
Q: GPU for small/medium-scale training/inference?
- A100  ✅
Correct Answer: A100
Explicación (español): Equilibrio de memoria y tensor cores.

### Q27 (oficial)
Q: Technique with explicit examples guiding LLM response?
- Few-shot prompting  ✅
Correct Answer: Few-shot prompting
Explicación (español): Ejemplos en el prompt para patrón.

### Q28 (oficial)
Q: Role of tokens in LLMs?
- Individual units into which text is divided  ✅
Correct Answer: Unidades mínimas para procesamiento.
Explicación (español): Segmentación a IDs/embeddings.

### Q29 (oficial)
Q: Model best to complete poem lines suggesting words?
- RNN  ✅
Correct Answer: RNN
Explicación (español): Manejo de contexto secuencial.

### Q30 (oficial)
Q: Pretraining process of Generative AI model?
- Learns patterns in unstructured data without labeled data  ✅
Correct Answer: Auto-supervisión en gran corpus.
Explicación (español): Predice tokens/fill-masks.

### Q31 (oficial)
Q: Purpose of hidden layer in ANN?
- Processes/transforms inputs via weights & activations  ✅
Correct Answer: Transformación no lineal.
Explicación (español): Extrae representaciones internas.

### Q32 (oficial)
Q: Locate & label vehicles and license plates per frame?
- Object detection  ✅
Correct Answer: Object detection
Explicación (español): Bounding boxes múltiples objetos.

### Q33 (oficial)
Q: AI subset for image classification identifying objects?
- Deep Learning  ✅
Correct Answer: Deep Learning
Explicación (español): CNN/Vision Transformers para imágenes.

### Q34 (oficial)
Q: Role of target variable in supervised learning?
- Desired output/class labels  ✅
Correct Answer: Etiquetas objetivo.
Explicación (español): Permite calcular la pérdida.

### Q35 (oficial)
Q: Categorize news articles (Politics/Tech/Sports)?
- Text classification  ✅
Correct Answer: Text classification
Explicación (español): Asigna categorías temáticas.

### Q36 (oficial)
Q: Primary limitation of RNNs on long sequences?
- Vanishing gradient long-range dependencies  ✅
Correct Answer: Vanishing gradient
Explicación (español): Gradiente se atenúa en secuencias largas.

### Q37 (oficial)
Q: Use pretrained AI models for vector search in DB 23ai?
- Loading ONNX models directly  ✅
Correct Answer: Modelos ONNX cargados en BD.
Explicación (español): Embeddings dentro de la base reducen latencia.

### Q38 (oficial)
Q: Network best for face recognition?
- CNN  ✅
Correct Answer: CNN
Explicación (español): Extrae rasgos espaciales jerárquicos.

### Q39 (oficial)
Q: Technique to reduce spam via automated filtering?
- Machine Learning  ✅
Correct Answer: Machine Learning
Explicación (español): Clasificación supervisada (spam/no spam).

### Q40 (oficial)
Q: Data type predicting stock prices?
- Time series data  ✅
Correct Answer: Time series data
Explicación (español): Serie temporal con orden cronológico.

---

## Preguntas No Oficiales

Contienen las primeras 9 preguntas originales del documento que no forman parte del set oficial:
Q1: NLP (sentiment & translation)
Q2: Model training (establecer relación input-output)
Q3: Propósito CNN (patrones en imágenes)
Q4: Diferencia Generative AI vs supervisado (modela distribución)
Q5: Ventaja Superclusters (alto rendimiento IA compleja)
Q6: In-context learning (ejemplos en prompt)
Q7: OCI Speech (transcribir voz a texto)
Q8: Document Understanding NO hace transcripción
Q9: Notebook Sessions entorno interactivo

---
## Ficha de Repaso Rápido

### Servicios Clave de OCI AI
- OCI Data Science: Entrenamiento, notebook sessions, model deployment, model catalog.
- OCI Vision: Clasificación de imágenes/documentos, OCR, extracción clave-valor, detección de objetos.
- OCI Speech: Transcripción, normalización, puntuación de confianza, soporte SRT, filtrado de lenguaje (tagging/masking/removing).
- OCI Language: Clasificación de texto, análisis de sentimiento, detección de idioma, NER.
- Oracle AI Vector Search / DB 23ai: Embeddings, búsqueda semántica, soporte ONNX dentro de la base.
- Generative AI (OCI): Fine-tuning eficiente (T-Few), in-context learning, prompting (zero / few-shot), manejo de tokens.

### Conceptos Fundamentales
- Overfitting: Excelente rendimiento en entrenamiento pero mala generalización.
- Inference: Uso del modelo entrenado para predecir en datos nuevos.
- Loss Function: (Implícito en varias preguntas) Mide el error entre predicción y objetivo para ajustar pesos.
- Embeddings: Representaciones vectoriales de significado semántico.
- Tokens: Unidades mínimas de texto para entrada/salida en LLM.
- Parameter-Efficient Fine-Tuning (T-Few): Ajustar solo fracción de pesos para reducir coste.
- RNN vs CNN: RNN secuencias / tiempo; CNN patrones espaciales.
- Time Series: Datos indexados temporalmente (stock prices).
- Key-Value Extraction: Pares estructurados (invoice, receipt fields).
- Confidence Scoring: Nivel de certeza por palabra en transcripción.

### Hardware & Infraestructura
- Superclusters: Alta densidad GPU + RDMA para entrenamiento masivo.
- GPUs: A100 (escala mediana), GB200 (exascala/HPC), A100 vs H200 (memoria diferenciada).
- RDMA: Baja latencia entre nodos para escalamiento distribuido.
- OCI Vault: Gestión de llaves y secretos (no componente de cómputo AI).

### Trustworthy AI
- Principios: Legal, Ética, Robusta.
- Mitigación de sesgos: Requiere dataset diverso, monitoreo de inferencia, explicación del modelo (no cubierto directamente pero relacionado).

### Tipos de Tareas
- Clasificación Supervisada: Spam, categorías de noticias.
- Clasificación de Imágenes: Uso de CNN/Deep Learning.
- Detección de Objetos: Bounding boxes múltiples (Vision Object Detection).
- Búsqueda Semántica: Comparación de embeddings (Vector Search).
- Generación Secuencial: Música, texto predictivo (RNN / Transformers).

### Errores y Limitaciones
- Vanishing Gradient (RNN): Dificulta dependencias largas.
- Duplicidad temática: Select AI aparece dos veces (generar SQL / interacción natural).

---
 
## Resumen Teórico (OCI AI Foundations)

Este apartado recoge de forma condensada los conceptos clave que caen en el examen 1Z0-1122-25 (AI Foundations Associate) y sirve como ficha de estudio rápida.

1) Fundamentos de la IA
- Simula procesos de inteligencia humana mediante algoritmos.
- Incluye tareas como reconocimiento de voz, visión por computadora y procesamiento de lenguaje natural (NLP).

2) Fundamentos del Aprendizaje Automático (ML)
### Q1
Q: AI Fundamentals - Which AI domain is associated with tasks like identifying the sentiment of a text and translating text between languages?

- A. Natural Language Processing  ✅
- B. Computer Vision
- C. Speech Processing
- D. Anomaly Detection

Correct Answer: A

Explicación (español):
NLP (Natural Language Processing) se encarga del procesamiento y análisis del lenguaje humano, incluyendo análisis de sentimiento y traducción automática.

---

### Q2
Q: Machine Learning Fundamentals - In Machine Learning, what does 'Model training' involve?

- A. Writing code for the entire program
- B. Collecting and labeling data
- C. Establishing a relationship between input and output parameters  ✅
- D. Analyzing the accuracy of a trained model

Correct Answer: C

Explicación (español):
El entrenamiento del modelo consiste en ajustar los parámetros del modelo para que aprenda la relación entre las entradas (features) y las salidas (labels) a partir de los datos.

---

### Q3
Q: Deep Learning Fundamentals - What is the primary purpose of Convolutional Neural Networks (CNNs)?

- A. Generating high-resolution images
- B. Creating music compositions
- C. Processing sequential data
- D. Detecting patterns in images  ✅

Correct Answer: D

Explicación (español):
Las CNN están diseñadas para extraer características espaciales de imágenes (bordes, texturas, formas) mediante convoluciones y pooling.

---

### Q4
Q: Generative AI Fundamentals - What distinguishes generative AI from supervised learning?

- A. Focuses on decision-making
- B. Understands data distribution to create new examples  ✅
- C. Generates labeled outputs for training
- D. Used only for text-based apps

Correct Answer: B

Explicación (español):
Generative AI modela la distribución de los datos y puede generar nuevas instancias similares; supervisado se centra en mapear entradas a etiquetas.

---

### Q5
Q: OCI AI Infrastructure - What advantage do OCI Superclusters offer for AI workloads?

- A. Ideal for text-to-speech
- B. Integrates with social media
- C. Cost-effective for simple tasks
- D. High performance for complex AI  ✅

Correct Answer: D

Explicación (español):
Ofrecen potencia y escalabilidad para entrenamiento e inferencia de modelos grandes (GPU, redes rápidas, almacenamiento alto rendimiento).

---

### Q6
Q: LLMs - What is 'in-context learning' in LLMs?

- A. Training a model on diverse tasks
- B. Modifying model permanently
- C. Zero-shot learning
- D. Providing examples via input prompt  ✅

Correct Answer: D

Explicación (español):
El modelo adapta su respuesta a una nueva tarea usando ejemplos en el prompt sin cambiar pesos.

---

### Q7
Q: OCI AI Services - Which feature does the OCI Speech service offer?

- A. Convert text to images
- B. Recognize objects
- C. Transcribe speech to text  ✅
- D. Analyze sentiment

Correct Answer: C

Explicación (español):
Convierte audio hablado en texto utilizable para análisis posterior y búsqueda.

---

### Q8
Q: OCI Document Understanding - What can OCI Document Understanding NOT do?

- A. Generate transcript  ✅
- B. Extract tables
- C. Classify documents
- D. Extract text

Correct Answer: A

Explicación (español):
No genera transcripciones de audio; realiza OCR, extracción de tablas y clasificación documental.

---

### Q9
Q: OCI Data Science - What feature provides an interactive coding environment for building and training models?

- A. ADS SDK
- B. Conda
- C. Model Catalog
- D. Notebook Sessions  ✅

Correct Answer: D

Explicación (español):
Entornos gestionados con recursos preconfigurados para desarrollar y entrenar modelos de forma interactiva.

---

### Q10
Q: Deep Learning - What is a key feature of RNNs?

- A. Parallel processing
- B. Used for images
- C. Feedback loop retains info across time steps  ✅
- D. No internal state

Correct Answer: C

Explicación (español):
Las conexiones recurrentes mantienen estado y capturan dependencias temporales en secuencias.

---

### Q11
Q: OCI Vector Search - What would you use Oracle AI Vector Search for?

- A. Store business data
- B. Manage security protocols
- C. Keyword queries
- D. Semantic queries  ✅

Correct Answer: D

Explicación (español):
Realiza búsqueda semántica comparando embeddings de significado, no simples palabras clave.

---

### Q12
Q: ML Concepts - What is 'overfitting'?

- A. Model is too simple
- B. Model learns training data too well  ✅
- C. Model ignores input data
- D. Model has no output

Correct Answer: B

Explicación (español):
Memoriza el conjunto de entrenamiento perdiendo capacidad de generalizar a datos nuevos.

---

### Q13
Q: Which algorithm is a non-parametric approach for supervised learning?

- Linear Regression
- K-Nearest Neighbors (KNN)  ✅
- Random Forest
- Decision Trees

Correct Answer: K-Nearest Neighbors (KNN)

Explicación (español):
No ajusta parámetros internos; usa distancias sobre datos almacenados. K es el hiperparámetro clave.

---

### Q14
Q: What is the purpose of the Model Catalog in OCI Data Science?

- It only stores raw datasets for training ML models.
- It is used to deploy models as API endpoints.
- It functions as a real-time data processing engine.
- It serves as a repository for storing, tracking, and managing machine learning models.  ✅

Correct Answer: It serves as a repository for storing, tracking, and managing machine learning models.

Explicación (español):
Centraliza versiones, metadatos y facilita colaboración y gobernanza del ciclo de vida del modelo.

---

### Q15
Q: What technique is used to predict the price of a house based on its features?

- Regression  ✅
- Time Series Analysis
- Classification
- Clustering

Correct Answer: Regression

Explicación (español):
Precio es variable continua -> problema de regresión; clasificación es para categorías, clustering sin etiquetas y series temporales para dependencias temporales.
 
---

### Q16
Q: John has successfully trained a machine learning model using OCI. He now needs to deploy it for real-time predictions where it can process user inputs and generate responses. Which OCI service should he use for deployment?

- OCI Object Storage
- OCI Language
- OCI Data Science  ✅
- OCI Speech

Correct Answer: OCI Data Science

Explicación (español):
OCI Data Science permite desplegar modelos como endpoints para inferencia en tiempo real (model deployment). Object Storage guarda datos/artefactos, OCI Language es para NLP gestionado y OCI Speech para voz; no sirven directamente para desplegar un modelo personalizado entrenado por John.

---

### Q17
Q: Which of these is NOT a common application of unsupervised machine learning?

- Spam detection  ✅
- Outlier detection
- Targeted marketing campaigns
- Customer segmentation

Correct Answer: Spam detection

Explicación (español):
La detección de spam normalmente es un problema supervisado (clasificación con ejemplos etiquetados spam/no spam). Outlier detection, campañas dirigidas (segmentación) y customer segmentation usan técnicas no supervisadas (clustering, análisis de densidad) para descubrir grupos o anomalías sin etiquetas previas.

---

### Q18
Q: Which OCI Vision feature is useful for identifying whether a document is an invoice, receipt, or resume, based on its appearance and keywords?

- Table extraction
- Image classification
- Document classification  ✅
- OCR (Optical Character Recognition)

Correct Answer: Document classification

Explicación (español):
Document Classification categoriza documentos por su estructura y contenido textual extraído. OCR extrae texto, pero no asigna tipo; Image classification sería demasiado genérica y Table extraction se enfoca en tablas.

---

### Q19
Q: T-Few fine-tuning in OCI Generative AI Service reduces cost and training time compared to traditional fine-tuning. Which statement correctly explains the reason behind it?

- It requires manually configuring each layer of the model for optimization.
- It trains the entire model from scratch for each task.
- It selectively updates only a fraction of the model's weights.  ✅
- It does not allow customization of pretrained models.

Correct Answer: It selectively updates only a fraction of the model's weights.

Explicación (español):
T-Few (parameter-efficient fine-tuning) ajusta solo capas o adaptadores adicionales en vez de todos los parámetros, reduciendo cómputo, tiempo y coste. No reentrena desde cero y sí permite personalización.

---

### Q20
Q: What type of data is most likely to be used with deep learning algorithms?

- Data with human interpretable features
- Complex data with non-human interpretable features  ✅
- Time series data
- Only string data

Correct Answer: Complex data with non-human interpretable features

Explicación (español):
Deep Learning destaca en datos complejos (imágenes, audio, vídeo) donde las características no son fácilmente interpretables de forma directa. También puede usarse en series temporales y texto, pero su fortaleza surge con representaciones complejas.

---

### Q21
Q: How does normalization improve the readability of transcriptions in OCI Speech?

- It translates transcriptions into multiple languages.
- It replaces all uppercase letters with lowercase.
- It converts elements like numbers, dates, and URLs into standard readable formats.  ✅
- It removes unnecessary words from the transcription.

Correct Answer: It converts elements like numbers, dates, and URLs into standard readable formats.

Explicación (español):
La normalización en transcripción transforma números, fechas, horas y URLs a un formato consistente y claro para posterior análisis. No traduce idiomas ni elimina palabras válidas.

---

### Q22
Q: David is transcribing a customer support call using OCI Speech. The call contains some profane language, and he wants to retain the original words but mark them as inappropriate rather than discarding them. Which profanity filtering option should he use?

- Tagging  ✅
- Removing
- Normalization
- Masking

Correct Answer: Tagging

Explicación (español):
La opción Tagging conserva las palabras originales y agrega anotaciones/etiquetas indicando que contienen lenguaje inapropiado. Removing las eliminaría, Masking las reemplazaría (por ejemplo, con asteriscos) y Normalization se orienta a estandarizar formatos (números, fechas), no a marcar insultos.

---

### Q23
Q: Mark is analyzing customer receipts and wants to automatically find and save details such as merchant name, transaction date, and total amount for record-keeping. Which OCI Vision feature should he use?

- OCR (Optical Character Recognition)
- Document classification
- Key-value extraction  ✅
- Table extraction

Correct Answer: Key-value extraction

Explicación (español):
Key-value extraction identifica campos predefinidos (pares clave:valor) dentro de documentos como recibos o facturas (ej. "Merchant: ACME", "Total: 45.90"). OCR solo extrae texto sin estructura; Document classification asigna tipo de documento; Table extraction detecta y extrae datos tabulares, no campos sueltos.

---

### Q24
Q: Which statement best describes the primary difference between Large Language Models (LLMs) and traditional Machine Learning (ML) models?

- Traditional ML models are better at understanding and generating natural language, while LLMs are primarily used for numerical data analysis.
- LLMs are pretrained on a large text corpus whereas ML models need to be trained on custom data.  ✅
- LLMs require extensive feature engineering, while traditional ML models can generate human-like text with minimal feature engineering.
- LLMs have a limited number of parameters compared to ML models.

Correct Answer: LLMs are pretrained on a large text corpus whereas ML models need to be trained on custom data.

Explicación (español):
Los LLM se entrenan previamente en corpora masivos y generalistas (pretraining), lo que les da capacidad para múltiples tareas con ajuste mínimo (prompting o fine-tuning ligero). Los modelos ML tradicionales suelen entrenarse desde cero o con menos transferencia usando datasets específicos de la tarea y requieren más preparación de features.

---

### Q25
Q: A self-driving car needs to detect pedestrians and make safe lane changes. Which AI concept is being applied here?

- Deep Learning
- Artificial Intelligence  ✅
- Machine Learning
- Natural Language Processing

Correct Answer: Artificial Intelligence

Explicación (español):
El escenario engloba múltiples capacidades cognitivas (percepción, decisión y planificación) simulando comportamiento humano: eso corresponde al nivel amplio de Inteligencia Artificial. Deep Learning y Machine Learning son subcampos que implementan componentes específicos (visión, predicción), pero el concepto general es AI.

---

### Q26
Q: John needs to analyze the accuracy of OCI Speech transcriptions for a legal case. He wants to evaluate how sure the model is about each word in the transcription. Which feature should he use?

- Normalization
- Confidence scoring  ✅
- Profanity filtering
- Batch support

Correct Answer: Confidence scoring

Explicación (español):
Confidence scoring proporciona puntuaciones de confianza por palabra y global, útiles para validar fiabilidad en contextos sensibles (legal, médico). Normalization estandariza formatos, profanity filtering gestiona lenguaje inapropiado y batch support se refiere al procesamiento de múltiples archivos.

---

### Q27
Q: You are working on a deep learning project to generate music. Which type of neural network is best suited for this task?

- Feedforward Neural Network (FNN)
- Recurrent Neural Network (RNN)  ✅
- Convolutional Neural Network (CNN)
- Autoencoder

Correct Answer: Recurrent Neural Network (RNN)

Explicación (español):
La generación musical requiere modelar secuencias donde cada nota depende del contexto previo; las RNN (y variantes LSTM/GRU o Transformers adaptados) manejan dependencias temporales. Una CNN se orienta a patrones espaciales, FNN no mantiene estado secuencial y un autoencoder se utiliza más para compresión/representación que para generación secuencial directa.

---

### Q28
Q: Emma works for a media company that produces video content for online platforms. She needs to add closed captions to their videos for accessibility. Which OCI Speech feature should Emma use?

- SRT file support  ✅
- Profanity filtering
- Batching support
- Confidence scoring

Correct Answer: SRT file support

Explicación (español):
El soporte de archivos SRT permite generar subtítulos en formato estándar ampliamente aceptado en plataformas de vídeo. Profanity filtering gestiona lenguaje inapropiado, batching procesa múltiples audios y confidence scoring ofrece niveles de confianza, pero ninguno genera directamente archivos de subtítulos estándar.

---

### Q29
Q: Which of these summarizes the three guiding principles for AI to be trustworthy?

- AI should be lawful, ethical, and robust.  ✅
- AI should replace human oversight, be independent, and highly scalable.
- AI should be cost-effective, cloud-based, and user-friendly.
- AI should be fast, unbiased, and autonomous.

Correct Answer: AI should be lawful, ethical, and robust.

Explicación (español):
La confiabilidad en AI se apoya en cumplir leyes (lawful), respetar valores éticos (ethical) y garantizar robustez técnica y social (robust). Las otras opciones mezclan atributos deseables pero no se consideran el marco normativo principal.

---

### Q30
Q: What is the primary function of the inference process in machine learning?

- Labeling the training data
- Predicting outcomes from new data points  ✅
- Collecting training data
- Adjusting the weights of a neural network

Correct Answer: Predicting outcomes from new data points

Explicación (español):
Inference aplica un modelo ya entrenado para generar predicciones sobre entradas nuevas. El ajuste de pesos sucede en entrenamiento; etiquetar datos y recolectarlos son pasos previos.

---

### Q31
Q: How does Select AI generate SQL queries from natural language questions?

- It provides the best SQL query based on predefined templates.
- It only works with structured datasets that contain predefined SQL commands.
- It requires users to manually input query parameters.
- It connects to an LLM, infers the query intent, and formulates the SQL command.  ✅

Correct Answer: It connects to an LLM, infers the query intent, and formulates the SQL command.

Explicación (español):
Select AI emplea modelos de lenguaje para interpretar la intención del prompt y construir la sentencia SQL adecuada sobre la base de datos. No depende únicamente de plantillas fijas ni requiere que el usuario defina manualmente todos los parámetros.

---

### Q32
Q: How does Select AI enhance the interaction with Oracle Autonomous Database?

- By improving network security protocols
- By eliminating the need for database administrators
- By enabling natural language prompts instead of SQL code  ✅
- By providing advanced data visualization tools

Correct Answer: By enabling natural language prompts instead of SQL code

Explicación (español):
Permite a usuarios formular consultas en lenguaje natural simplificando acceso a datos sin escribir SQL manualmente. (Nota: Esta pregunta es equivalente a una anterior; se mantiene para práctica adicional).


---

## Preguntas adicionales (Usuario 24–30, pendientes de reorganización)

Estas preguntas se han añadido según tu listado reciente (numeración 24–30 del usuario). Mantengo la numeración original con prefijo U para evitar conflictos temporales con las Q existentes (Q1–Q32). Luego las integraremos y renumeraremos según el esquema final (oficial vs práctica). Se han corregido respuestas donde el marcado original parecía inconsistente con la explicación.

### U24
Q: Which of these components is NOT a part of OCI AI Infrastructure?

- OCI Storage
- RDMA Network
- NVIDIA GPUs
- OCI Vault  ✅

Correct Answer: OCI Vault

Explicación (español):
OCI AI Infrastructure se refiere a recursos de cómputo acelerado (GPU NVIDIA), almacenamiento de alto rendimiento y red RDMA de baja latencia para escalamiento masivo de entrenamiento e inferencia. OCI Vault es un servicio de gestión de claves y secretos (seguridad), no un componente de cómputo/infrastructura de AI directa.

---

### U25
Q: You need a suitable GPU for massive-scale (HPC) AI training and inference workloads. Which NVIDIA GPU are you most likely to choose?

- H100
- GB200  ✅
- A10
- A100

Correct Answer: GB200

Explicación (español):
La arquitectura Grace Blackwell (GB200) está diseñada para cargas exascala/HPC y entrenar modelos de gran tamaño con enormes requisitos de memoria y ancho de banda, superando a generaciones previas (A100, H100) para escenarios de escalamiento extremo.

---

### U26
Q: You need a suitable GPU for small or medium-scale AI training and inference workloads. Which NVIDIA GPU are you most likely to choose?

- GB200
- H200
- A100  ✅
- (Posible typo "8200" en texto original)  

Correct Answer: A100

Explicación (español):
La GPU NVIDIA A100 se emplea ampliamente para entrenamiento e inferencia de tamaño pequeño a mediano gracias a su equilibrio entre núcleos Tensor, memoria y soporte multi-instance GPU (MIG). GB200/H200 apuntan a escalas mayores o mayor capacidad de memoria; A10 es más orientada a inferencia ligera y gráficos.

Nota: El texto original parecía marcar H200, pero la explicación proporcionada indicaba A100 como opción para estos casos; se ha ajustado la respuesta.

---

### U27
Q: Which technique involves providing explicit examples in a prompt to guide an LLM's response?

- Few-shot prompting  ✅
- Chain-of-thought prompting
- Self-supervised learning
- Zero-shot learning

Correct Answer: Few-shot prompting

Explicación (español):
Few-shot prompting incluye varias demostraciones (k ejemplos) dentro del prompt para que el LLM infiera el patrón de la tarea antes de producir la respuesta nueva. Zero-shot no aporta ejemplos; Chain-of-thought promueve razonamiento paso a paso; Self-supervised es un paradigma de entrenamiento, no una técnica de prompt.

---

### U28
Q: What role do tokens play in Large Language Models (LLMs)?

- They are used to define the architecture of the model's neural network.
- They represent the numerical values of model parameters.
- They determine the size of the model's memory.
- They are individual units into which a piece of text is divided during processing by the model.  ✅

Correct Answer: They are individual units into which a piece of text is divided during processing by the model.

Explicación (español):
Los tokens son piezas mínimas (palabras, subpalabras o caracteres) en que se segmenta el texto para transformar a IDs e embeddings. El modelo opera sobre secuencias de tokens, no sobre oraciones completas directamente.

---

### U29
Q: You are writing poems. You need your computer to help you complete your lines by suggesting the right words. Which deep learning model is best suited for this task?

- Generative Adversarial Network (GAN)
- Recurrent Neural Network (RNN)  ✅
- Convolutional Neural Network (CNN)
- Variational Autoencoder (VAE)

Correct Answer: Recurrent Neural Network (RNN)

Explicación (español):
Las RNN (y hoy en práctica Transformers) manejan dependencias secuenciales: cada palabra depende del contexto previo, lo que permite sugerencias coherentes. GAN/VAE se orientan más a generación de muestras (imágenes, latentes) sin manejo explícito de dependencia temporal; CNN prioriza patrones espaciales.

Nota: El marcado original parecía seleccionar GAN; se corrige a RNN.

---

### U30
Q: Which statement best describes the pretraining process of a Generative AI model?

- It exclusively relies on reinforcement learning from human feedback.
- It only memorizes examples and reproduces them exactly.
- It learns from labeled data and maps inputs to corresponding labels.
- It learns patterns in unstructured data without requiring labeled training data.  ✅

Correct Answer: It learns patterns in unstructured data without requiring labeled training data.

Explicación (español):
El pretraining se realiza de forma auto-supervisada sobre grandes corpora no etiquetados (ej. predicción de la siguiente palabra/máscara). RLHF puede aplicarse en etapas posteriores de ajuste fino para alineación, no es la base exclusiva. No memoriza literalmente sino modela distribuciones.

---

### U31
Q: What is the purpose of the hidden layer in an artificial neural network?

- This layer is optional, and it processes and transforms inputs from the network's weights and activation functions.  ✅
- This layer receives and passes input data to the subsequent layers; it does not perform any computations.
- This layer applies filters to the image data.
- This layer produces the final outputs of the network based on the processed information from the other layers.

Correct Answer: This layer is optional, and it processes and transforms inputs from the network's weights and activation functions.

Explicación (español):
Las capas ocultas realizan transformaciones no lineales sobre las entradas (o salidas de capas previas) mediante pesos y funciones de activación, permitiendo al modelo aprender representaciones complejas. No generan la salida final (eso lo hace la capa de salida) y sí realizan cómputos esenciales.

---

### U32
Q: Lisa runs an automated security system that monitors parking lots using cameras. She wants to locate and label vehicles and license plates in each frame. Which OCI Vision feature should she use?

- Image classification
- Object detection  ✅
- Speech-to-text
- Document Classification

Correct Answer: Object detection

Explicación (español):
Object detection identifica y dibuja bounding boxes alrededor de múltiples objetos (vehículos, matrículas) en cada imagen o frame. Image classification solo asigna una etiqueta global; Speech-to-text no aplica y Document Classification es para tipo de documento (factura, recibo, etc.).

---

### U33
Q: You're developing an image classification software that can identify specific objects. Which AI subset would you use?

- Reinforcement Learning
- Machine Learning
- Deep Learning  ✅
- Natural Language Processing

Correct Answer: Deep Learning

Explicación (español):
Para clasificación de imágenes se usan típicamente arquitecturas de Deep Learning (CNN, Vision Transformers). Machine Learning tradicional podría emplearse con features manuales, pero el estándar actual es DL. Reinforcement Learning se orienta a decisiones secuenciales y NLP al lenguaje.

---

### U34
Q: What is the role of a target variable in supervised learning?

- It helps in feature selection.
- It represents the input data.
- It is used to split the dataset.
- It contains the desired output or class labels.  ✅

Correct Answer: It contains the desired output or class labels.

Explicación (español):
La variable objetivo (label) es el valor que el modelo debe predecir (clase, valor numérico). Permite calcular la pérdida comparando predicción vs ground truth. No se usa directamente para dividir el dataset (eso lo hace un criterio externo) y su función principal no es seleccionar features.

---

### U35
Q: John works in a news aggregation platform and wants to automatically categorize articles into topics like "Politics", "Technology", and "Sports". Which feature of OCI Language would help him?

- Language detection
- Text classification  ✅
- Sentiment analysis
- Named entity recognition

Correct Answer: Text classification

Explicación (español):
La clasificación de texto asigna documentos a categorías entre un conjunto amplio (>600) soportado por OCI Language. Language detection solo identifica idioma, sentiment analiza polaridad emocional y NER extrae entidades (personas, lugares) pero no clasifica el tema global.

---

### U36
Q: What is the primary limitation of Recurrent Neural Networks (RNNs) when processing long sequences?

- RNNs can only process numerical data, not text.
- RNNs struggle with long-range dependencies due to the vanishing gradient problem.  ✅
- RNNs process words in parallel, making them less efficient than transformers.
- RNNs can only understand words that appear at the beginning of a sentence.

Correct Answer: RNNs struggle with long-range dependencies due to the vanishing gradient problem.

Explicación (español):
El gradiente se atenúa (o explota) al propagarse por muchos pasos, dificultando aprender dependencias lejanas. LSTM/GRU mitigan parcialmente, Transformers eliminan la dependencia estricta secuencial con atención global.

---

### U37
Q: How does Oracle Database 23ai allow the use of pretrained AI models for vector search?

- By restricting models to only Oracle-provided APIs
- By storing only raw image files for direct comparison
- By manually converting AI models into SQL queries
- By loading ONNX models directly into the database  ✅

Correct Answer: By loading ONNX models directly into the database

Explicación (español):
Permite integrar modelos preentrenados (formato abierto ONNX) para generar embeddings y realizar búsquedas vectoriales dentro del motor, reduciendo latencia y evitando movimiento excesivo de datos.

---

### U38
Q: You are training a deep learning model to recognize faces. What type of neural network is best suited for this task?

- Autoencoder
- Convolutional Neural Network (CNN)  ✅
- Feedforward Neural Network (FNN)
- Recurrent Neural Network (RNN)

Correct Answer: Convolutional Neural Network (CNN)

Explicación (español):
Las CNN extraen características espaciales jerárquicas (bordes, texturas, rasgos faciales) cruciales para reconocimiento. RNN se orienta a secuencias temporales, FNN carece de inductive bias espacial y Autoencoders son más para reconstrucción/compresión.

---

### U39
Q: A company wants to automate its email filtering system to reduce spam. Which AI technique would you recommend?

- Reinforcement Learning
- Natural Language Processing
- Deep Learning
- Machine Learning  ✅

Correct Answer: Machine Learning

Explicación (español):
El filtrado de spam es un caso clásico de clasificación supervisada (features del correo -> etiqueta spam/no). NLP describe el dominio; Deep Learning puede ser la técnica concreta dentro de ML, pero la respuesta general correcta es usar ML supervisado (modelos como Naive Bayes, SVM, Transformers). Reinforcement Learning no aplica aquí.

---

### U40
Q: You are training a deep learning model to predict stock prices. What type of data is this an example of?

- Sequential data
- Text data
- Time series data  ✅
- Image data

Correct Answer: Time series data

Explicación (español):
Los precios organizados por timestamp conforman una serie temporal (time series) con orden cronológico y dependencias autocorrelacionadas. Es un caso especial de datos secuenciales, pero la categoría precisa es series temporales.

---

## Traducción Completa Español

Esta sección reagrupa todas las preguntas traducidas al español separando: Oficiales (Q1–Q40) y Opcionales (O1–O9). Se eliminan prefijos mezclados y se mantiene una referencia clara.

### Oficiales (Q1–Q40)
1. Q1: Dominio IA para sentimiento y traducción -> NLP.
2. Q2: Entrenamiento del modelo -> Relación entrada-salida (ajuste de parámetros).
3. Q3: Propósito principal CNN -> Detectar patrones en imágenes.
4. Q4: Diferencia IA generativa vs supervisado -> Modela distribución y crea nuevos ejemplos.
5. Q5: Ventaja Superclusters OCI -> Alto rendimiento para IA compleja.
6. Q6: In-context learning -> Ejemplos en el prompt sin cambiar pesos.
7. Q7: Función clave OCI Speech -> Transcribir voz a texto.
8. Q8: OCI Document Understanding NO hace -> Transcripción de audio.
9. Q9: Entorno interactivo OCI Data Science -> Notebook Sessions.
10. Q10: Característica clave RNN -> Bucle que retiene información temporal.
11. Q11: Uso Vector Search -> Consultas semánticas.
12. Q12: Overfitting -> Memoriza entrenamiento, no generaliza.
13. Q13: Algoritmo no paramétrico -> KNN.
14. Q14: Propósito Model Catalog -> Repositorio y gestión de modelos.
15. Q15: Técnica para precio de casa -> Regresión.
16. Q16: Servicio despliegue modelo -> OCI Data Science.
17. Q17: NO aplicación no supervisado -> Detección de spam.
18. Q18: Vision para tipo de documento -> Clasificación de documentos.
19. Q19: Razón ahorro T-Few -> Actualiza fracción de pesos.
20. Q20: Datos típicos DL -> Datos complejos no interpretable directamente.
21. Q21: Normalización Speech -> Formatos estándar (números, fechas, URLs).
22. Q22: Profanity que marca y mantiene -> Tagging.
23. Q23: Vision extrae campos recibos -> Extracción clave-valor.
24. Q24: Diferencia LLM vs ML trad. -> Preentrenado gran corpus.
25. Q25: Concepto coche autónomo -> Inteligencia Artificial.
26. Q26: Evaluar certeza transcripción -> Confidence scoring.
27. Q27: Red para música secuencial -> RNN.
28. Q28: Subtítulos accesibles -> Soporte SRT.
29. Q29: Principios AI confiable -> Legal, ética, robusta.
30. Q30: Función inferencia -> Predecir en datos nuevos.
31. Q31: Generar SQL Select AI -> LLM infiere intención y construye SQL.
32. Q32: Mejora interacción Select AI -> Prompts lenguaje natural.
33. Q33: Subconjunto para clasificación de imágenes -> Deep Learning.
34. Q34: Rol variable objetivo -> Etiqueta salida deseada.
35. Q35: Clasificar artículos noticia -> Clasificación de texto.
36. Q36: Limitación RNN secuencias largas -> Vanishing gradient.
37. Q37: Vector search DB 23ai -> Cargar modelos ONNX.
38. Q38: Red para reconocimiento facial -> CNN.
39. Q39: Técnica reducir spam -> ML supervisado.
40. Q40: Tipo de datos precios acciones -> Serie temporal.

### Opcionales (O1–O9)
1. O1: Dominio IA (sentimiento/traducción) -> NLP.
2. O2: Entrenamiento modelo -> Relación entrada-salida.
3. O3: Propósito CNN -> Patrones espaciales imágenes.
4. O4: Generative vs supervised -> Modela distribución para nuevos ejemplos.
5. O5: Ventaja Superclusters -> Alto rendimiento IA.
6. O6: In-context learning -> Ejemplos en prompt.
7. O7: OCI Speech -> Transcripción voz-texto.
8. O8: Document Understanding NO -> Transcribir audio.
9. O9: Entorno interactivo DS -> Notebook Sessions.

Nota: Las opcionales refuerzan conceptos duplicados de las oficiales; se pueden usar para prácticas de repetición espaciada.

----
Fin de la sección de traducción completa.

----
## Uso y Recomendaciones de Estudio

1. Repaso Activo: Lee cada pregunta oficial (Q1–Q40) intentando responder antes de ver la opción marcada.
2. Espaciado: Divide en bloques de 10 preguntas y repite al día siguiente para reforzar memoria.
3. Agrupación Temática: Estudia juntas Vision (Q9, Q14, Q32, Q33, Q38, Q23), Speech (Q12, Q13, Q17, Q19, Q21, Q28, Q26), Language (Q35, U35 en traducción), Generative (Q10, Q15, Q22, Q23, Q27–Q30).
4. Flashcards: Crea tarjetas para términos clave (Overfitting, Embeddings, Tokens, T-Few, Confidence Scoring, Key-Value Extraction).
5. Profundización: Para cada servicio, visita documentación oficial de OCI y relaciona casos de uso prácticos (ej. facturas reales para Vision, logs de soporte para Language).
6. Último Día: Realiza un pase completo de las 40 oficiales sin mirar respuestas; luego corrige y refuerza las falladas.

Checklist Final de Dominio (auto-evaluación):
- [ ] Diferencias entre infra (Superclusters) y servicios gestionados (Data Science, Vision, Speech, Language).
- [ ] Flujo completo ML: datos -> entrenamiento -> despliegue -> inferencia -> monitoreo.
- [ ] Técnicas de prompting (zero-shot vs few-shot vs in-context).
- [ ] Distinción entre clasificación de documento, OCR, extracción clave-valor y detección de objetos.
- [ ] Principios de AI confiable y su aplicación práctica.
- [ ] Ventaja de parameter-efficient fine-tuning.
- [ ] Limitaciones de RNN y rol de Transformers (implicado).

Sugerencia de Mejora Futuro: Añadir una sección de preguntas abiertas y escenarios prácticos (no incluida aquí) para razonamiento profundo.

---



